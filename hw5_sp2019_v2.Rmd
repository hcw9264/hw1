---
title: "Home Work 5 (Updated) (Points: 154)"
subtitle: (APANPS5335 Machine Learning)
author: "Huan Cheng Wang"
 - "Submitted by: YOUR NAME HERE"
 - "hw2647"
date: "Posted: 4/3/2019 | Due: 4/22/2019"
output:
  pdf_document: default
  html_document:
    number_sections: no
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE,
                      tidy=TRUE, tidy.opts=list(width.cutoff=80))
```


__Instructions__: Please submit both the RMarkdown file and a PDF version. Make sure to complete the name and UNI in the header of this document.

__Note:__ Use `set.seed(19)` throughout the assignment whenever needed. Particularly, you should set the seed when making train and test partitions. This helps with grading the homework.


# Question 1 (50 points)

This problems deals with applying variable selection technique in regression. 

# 1.1 (2 points)

Generate a predictor $X$ of length $n=200$, as well as noise vector $\varepsilon$, of length $n = 200.$ Use stanndard normal random variate generator in R to generate the numbers. 

```{r}
# Your code goes here
set.seed(19)
x5q1=rnorm(n=200)
x5q1noise=rnorm(n=200)

```

# 1.2 (2 points)

Generate a response vector $Y$ of length $n= 200$ according to the following model:

\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \varepsilon,
\]

where $\beta_0 = 2$, $\beta_1 = 0.5$, $\beta_2 = 3$, and $\beta_4**** = 0.25.$
```{r}

beta_0=2
beta_1=0.5
beta_2=3
beat_3=0.25
set.seed(19)

y5q1=beta_0+beta_1*x5q1+beta_2*(x5q1^2)+beat_3*(x5q1^3)+x5q1noise

```
# 1.3 (13 points)

Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors $X, X^2, X^3, \ldots, X^{10}.$. What is the best model obtained according to $C_p$, BIC, and adjusted R^2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. 


```{r}
# RUBRIC: 
# Model fitting: 4 points
# Finding Cp, BI, and adj R^2: 3 points
# Plots: 3 points
# Reporting and discussion: 3 points

# Your code goes here. Create as many r-chuncks as necessary. Do not put all your R codes in one chunk.
library(leaps)

#1) Create a dataframe 200 rows and 1 col (x)
df5q1<-data.frame(y5q1,x5q1)

#2) Divide up dataframe in train and test
require(caTools)
set.seed(19)
sample=sample.split(df5q1$y5q1, SplitRatio = .8)
train=subset(df5q1, sample == TRUE)
test=subset(df5q1, sample == FALSE)

#3) Perform regsubsets
subsets<- regsubsets(y5q1~poly(x5q1,10), data=train, nvmax=10)
summary(subsets)


#4) Calculate cp, bi, adj r2
subsets_measures = data.frame(model=1:length(summary(subsets)$cp),cp=summary(subsets)$cp,bic=summary(subsets)$bic, adjr2=summary(subsets)$adjr2)

subsets_measures

#5) Plots
library(ggplot2)
library(tidyr)

subsets_measures %>%
  gather(key = type, value=value, 2:4)%>%
  ggplot(aes(x=model,y=value))+
  geom_line()+
  geom_point()+
  facet_grid(type~.,scales='free_y')

#6) Reporting and discussion
##enter your analysis based on best model's bic, cp and adj r2##



#find the model with lowest cp, bic and highest adjr2
##cp
which.min(summary(subsets)$cp)
##BIC
which.min(summary(subsets)$bic)
## adjr2
which.max(summary(subsets)$adjr2)

#find coefficients for model with lowest cp
coef(subsets,which.min(summary(subsets)$cp))

#do same thing for bic and adj r2 and pick your winning model
coef(subsets,which.min(summary(subsets)$bic))
coef(subsets,which.min(summary(subsets)$adjr2))
```

# 1.4 (11 points)

Repeat 1.3 using forward stepwise selection adn also using backwards stepwise selection.  How does your answer compare wto the results in 1.3?

```{r}
# RUBRIC: 
# Model fitting: 4 + 4 points
# Reporting and discussion: 3 points

# Your code goes here

#1) Model fitting using forward stepwise
start_mod = lm(y5q1~1,data=train)
empty_mod = lm(y5q1~1,data=train)
full_mod = lm(y5q1~poly(x5q1,10),data=train)
forwardStepwise = step(start_mod,scope=list(upper=full_mod,lower=empty_mod),direction='forward')
summary(forwardStepwise)

#2) Model fitting using backward stepwise
start_mod = lm(y5q1~poly(x5q1, 10),data=train)
empty_mod = lm(y5q1~1,data=train)
full_mod = lm(y5q1~poly(x5q1, 10),data=train)
backwardStepwise = step(start_mod,scope=list(upper=full_mod,lower=empty_mod),direction='backward')
summary(backwardStepwise)

#3) Reporting and discussion
##backward stepwise and forward stepwise produces the exact same results. 

####----codes----
which.min(summary(forwardStepwise)$cp)
which.min(summary(forwardStepwise)$bic)
which.max(summary(forwardStepwise)$adjr2)
which.min(summary(backwardStepwise)$cp)
which.min(summary(backwardStepwise)$bic)
which.max(summary(backwardStepwise)$adjr2)
#find coefficients of best model, similar to 1.3

forwardStepwise$coefficients
backwardStepwise$coefficients
```

# 1.5 (13 points)

Now fit a lasso model to the data set in 1.1, using $X, X^2, X^3, \ldots, X^{10}$ as predictors. Use cross-validation to select the optimal value of $\lambda.$ Create plots of the cross-validation error as a function of $\lambda.$ 

Report the resulting coefficient estimates, and discuss the results obtained.

```{r}
# RUBRIC: 
# Model fitting: 4 points
library(glmnet)
m = model.matrix(y5q1~poly(x5q1,10)-1,data=train)
y = train$y5q1
lassoModel = glmnet(m,y, alpha=1)
lassoModel

plot(lassoModel,xvar='lambda',label=T)

# Finding optimum lambda: 3 points
cv.lasso = cv.glmnet(m,y,alpha=1) # 10-fold cross-validation

# Plots: 3 points
plot(cv.lasso)

# Reporting and discussion: 3 points
coef(cv.lasso)

```


# 1.6 (14 points)

Now generate a response vector $Y$ according to the model 
\[
Y = \beta_0 + \beta_7 X^7 + \varepsilon,
\]
and perform best subset selection and the lasso. Discuss the results obtained.

```{r}
# Your code goes here. Do not put all the R codes in one chunk. Use as many chunks as needed to present nicely.
##what is beta_7?
y5q16=beta_0+
  
# RUBRIC: 
# Model fitting: 4 + 4 points
# Reporting and discussion: 6 points

##same as 1.3-1.4 and 1.5





```


# Question 2 (17 points)

We will use the `College` data set in ISLR package. 

```{r}
library(ISLR)
df5q2 = College
dim(df5q2)

df5q2
```


# 2.1 (1 point)

Split the data set into training and a test set using 70/30 ratio.

```{r}
set.seed(19)

dfShuffle5q2= df5q2[sample(nrow(df5q2)),]

summary(dfShuffle5q2)
nrow(dfShuffle5q2)

dfShuffle5q2testNum=round(nrow(dfShuffle5q2)*.7)
dfShuffle5q2trainNum=round(nrow(dfShuffle5q2)*.3)

df5q2Train=dfShuffle5q2[1:dfShuffle5q2trainNum,]
df5q2Test=dfShuffle5q2[(dfShuffle5q2testNum+1):dfShuffle5q2trainNum,]

```

# 2.2 (4 points)

Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
# R code


#confirm which one is the dependent variable in the dataset #

#fit lm using least squares on train set
lmdf5q2=lm(Accept~.-1, data=df5q2Train) #added -1

summary(lmdf5q2)
lmdf5q2_pred=predict(lmdf5q2, newdata=df5q2Test)

#calculate test error
mean((df5q2Test$Accept- lmdf5q2_pred)^2)

```


# 2.3 (4 points)

Fit a ridge regression odel on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

```{r}
# R code
library(glmnet)
as.matrix(df5q2Train)
cv.ridge.df5q2=cv.glmnet(df5q2Train[,c(2,4,5,6,17)],df5q2Train[,3],alpha=0)
y.pred.df5q2=predict(cv.ridge.df5q2,df5q2Test[,c(c,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18)])
mean((y.pred.df5q2-df5q2Test[,3])^2)
```

# 2.4 (4 points)

Fit a lasso model on the training set, with $\lambda$ chosen by cross-valiation. Report the test error obtained, along with the number of non-zero coefficient estimates.
```{r}
cv.lasso.df5q2=cv.glmnet(df5q2Train[,c(2,4,5,6,17)],df5q2Train[,3],alpha=1)
y.pred.df5q2=predict(cv.lasso.df5q2,df5q2Test[,c(c,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18)])
mean((y.pred.df5q2-df5q2Test[,3])^2)
```
# 2.5 (4 points)

Compare and comment on the results obtained using ridge regression and the lasso models with respect to how accurately can we predict the number of college applications received.



# Question 3 (37 points points)

Use the `OJ` data set in the `ISLR` package to answer the following problems. 
```{r}
df5q3 = OJ
dim(df5q3)

```


# 3.1 (2 points)

Create a training set containing 70\% of the observations in the data. The test set containing the remaining 30\% observations.


```{r}
# load the data

dfShuffle5q3= df5q3[sample(nrow(df5q3)),]

summary(dfShuffle5q3)
nrow(dfShuffle5q3)

dfShuffle5q3testNum=round(nrow(dfShuffle5q3)*.7)
dfShuffle5q3trainNum=round(nrow(dfShuffle5q3)*.3)

df5q3Train=dfShuffle5q3[1:dfShuffle5q3trainNum,]
df5q3Test=dfShuffle5q3[(dfShuffle5q3testNum+1):dfShuffle5q3trainNum,]
```

# 3.2 (10 points)

Fit a tree to the training data, with `Purchase` as the response and the other variables except for `Buy` as the predictors. Use the `summary()` function to preduce summary statistics about the tree, and describe the results obtained. 

What is the training error rate? How many terminal nodes does the tree have?


```{r}
# RUBRIC: 
# Fit the tree: 2
library(rpart)
OJ_tree=rpart(Purchase~.,data=df5q3Train)
# summary: 2 
summary(OJ_tree)
# description of the summary: 2
##LoyalCH scores the highest on Variable Importance
##Split number increases
##rel error usually < x error
# Training error rate: 2
library(caret)
postResample(df5q3Test,df5q3Train$Purchase)
# Number of terminal nodes of the tree: 2
39
```

# 3.3 (4 points)

Plot the tree and interprer the results.

```{r}
# RUBRIC: 
# Plot the tree: 2
library(rpart.plot)
rpart.plot(OJ_tree)
# Interpretation: 
##5 filters, 

```

# 3.4 (6 points)

Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

```{r}
# RUBRIC: 
# Perdicted values: 2
Pred_OJ_tree=predict(OJ_tree,df5q3Test)

# Confusion matrix: 2
library(caret)

CM_OJ_tree=table(df5q3$Purchase,df5q3Test)
# Test error rate: 2
postResample(df5q3Train,df5q3Test$Purchase)
```

# 3.5 (2 points)

Apply the `cv.tree()` function to the training set in order to determine the obtimal tree size.

```{r}
# RUBRIC: 
# cv.tree() application: 1
library(tree)
##Error in cv.tree(OJ_tree, , prune.tree) : not legitimate tree
cv.tree(OJ_tree,,prune.tree)

# Optimal tree size: 1

```

# 3.6 (6 points)

Plot the tree size on the x-axis and cross-validated classification error rate on the y-axis. Which tree size corresponds to the lowest cross-validatied classification error rate?

```{r}
# RUBRIC: 
# Plot: 4
# Optimal tree size: 2
```

# 3.7 (2 points)

Produce a pruned tree corresponding to tbe optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terinal nodes.

```{r}
# RUBRIC: 

```

# 3.8 (5 points)

Compare the training error rates and test error rates between the pruned and unpruned trees. Which is higher? Comment on the results.

```{r}
# RUBRIC: 
# Training error rate: 2

# Test error rate: 2

# Comment on the results: 1
```





# Question 4 (50 points)

Please see lecture slide 11 to understand the context of this problem. Pages 22 - 27

We so far did not use the qualitative variables in credit data (e.g., Gender, Student, Married, Ethnicity)

# 4.1 (10 points)

Analyze the data now using Neural Net introducing dummy variables corresponding to the categorical variables
```{r}
df5q4=Credit[,c(2,3,4,5,6,7,12)]
df5q4SampleSize=0.60 * nrow(df5q4)
df5q4index= sample(seq_len(nrow(df5q4)),size=df5q4SampleSize)

df5q4Train = df5q4[df5q4index,]
df5q4Test = df5q4[-df5q4index,]

df5q4Max=apply(df5q4,2,max)
df5q4Min=apply(df5q4,2,min)
df5q4Scaled=data.frame(scale(df5q4,center = df5q4Min,scale = df5q4Max-df5q4Min))

df5q4trainNN=df5q4Scaled[df5q4index,]
df5q4testNN=df5q4Scaled[-df5q4index,]

library(neuralnet)
df5q4NN=neuralnet(Rating~Income+Limit+Cards+Age+Education+Balance,df5q4trainNN,hidden=3,linear.output = 3)
```

# 4.2 (5 points)

Measure the performance and compare it on test data.
```{r}
df5q4Rateing=which(colnames(df5q4)=="Rating")
df5q4PredTestNN=compute(df5q4NN,df5q4testNN[,-(which(colnames(df5q4)=="Rating"))])
df5q4PredTestNN2=(df5q4PredTestNN$net.result*(max(df5q4$Rating)-min(df5q4$Rating))+min(df5q4$Rating))

```

# 4.3 (30 points)

Bootstrap estimate: We have not yet looked at the robustness of our models. Perform bootstrap with 100 draws on training data, and estimate the performance of the model on the training data which was not selected in the bootstrap sample. Compute the estimate of RMSE and 90\% confidence intervals.
```{r}
library(plyr)
n5q4=length(df5q4Train)
b5q4=100
x_temp5q4=rnorm(n5q4)
bootstrapMean5q4=rep(0,b5q4)

for (i in i:b5q4) {
  x.bootstrap=sample(x_temp5q4,n5q4,replace=T)
  bootstrapMean5q4[i]=mean(x.bootstrap)
}
mu5q4=mean(bootstrapMean5q4)
mean.bar5q4=mean((bootstrapMean5q4-mu5q4)^2)

#worked before but somehow lost the normal distribution
hist(bootstrapMean5q4)

muSort5q4=sort(bootstrapMean5q4)
mu94CI5q4

```
# 4.4 (5 points)

How does the estimated RMSE from bootstrap compare with RMSE on test data?
```{r}
rate_pos5q3=which(colnames(df5q4)=="Rating")
predict_testNN5q3=compute(NN,)
RMSE.NN5q4=(sum((df5q4Test$Rating-df5q4PredTestNN[,-()])))
```

```{r}
credit=Credit[,c(2,3,4,5,6,7,12)]
# Random sampling
samplesize = 0.60*nrow(Credit)
index=sample(seq_len(nrow(credit)),size=samplesize)
# Create training and test set
credittrain = credit[ index, ]
credittest = credit[ -index, ]
## Scale data for neural network
max = apply(credit , 2 , max)
min = apply(credit, 2 , min)
scaled = as.data.frame(scale(credit, center = min, scale = max - min))
# creating training and test set
trainNN = scaled[index , ]
testNN = scaled[-index , ]
# Fit neural network
library(neuralnet)
NN = neuralnet(Rating~Income+Limit+Cards+Age+Education+Balance,
trainNN, hidden = 3 , linear.output = T )
# plot neural network
plot(NN)
plot(trainNN$Rating,NN$response)
#Now prediction on the original scale with test data
## Prediction using neural network
rate_pos<-which(colnames(credit)=="Rating")
predict_testNN =compute(NN, testNN[,-(which(colnames(credit)=="Rating"))])
predict_testNN=(predict_testNN$net.result*(max(credit$Rating)-min(credit$Rating))-min(credit$Rating))+min(credit$Rating)
plot(credittest$Rating, predict_testNN, col="blue", pch=16,ylab = "predicted ratingNN", xlab = "real rating")
abline(0,1)
# Calculate Root Mean Square Error (RMSE)
RMSE.NN = (sum((credittest$Rating - predict_testNN)^2)/nrow(credittest)) ^ 0.5
```

